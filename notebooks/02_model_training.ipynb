{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein FT-Transformer: Model Training Notebook\n",
    "\n",
    "This notebook demonstrates how to train the Protein FT-Transformer model for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from models.ft_transformer import ProteinFTTransformer\n",
    "from training.dataloader import DataProcessor\n",
    "from training.trainer import Trainer\n",
    "from evaluation.metrics import MetricsCalculator\n",
    "from evaluation.visualization import plot_training_history, plot_confusion_matrix\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'data': {\n",
    "        'train_path': '../data/train.csv',\n",
    "        'test_path': '../data/test.csv',\n",
    "        'validation_split': 0.1,\n",
    "        'target_column': 'target',\n",
    "        'categorical_columns': [],\n",
    "        'numeric_columns': []\n",
    "    },\n",
    "    'model': {\n",
    "        'd_token': 128,\n",
    "        'n_head': 8,\n",
    "        'n_layers': 6,\n",
    "        'dropout': 0.2,\n",
    "        'ff_dim_factor': 4\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 64,\n",
    "        'num_epochs': 50,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 0.0001,\n",
    "        'gradient_clip': 1.0,\n",
    "        'early_stopping_patience': 10,\n",
    "        'early_stopping_min_delta': 0.001,\n",
    "        'save_best_only': True,\n",
    "        'use_mixed_precision': True\n",
    "    },\n",
    "    'loss': {\n",
    "        'focal_gamma': 2.0,\n",
    "        'label_smoothing': 0.1,\n",
    "        'use_class_weights': True\n",
    "    },\n",
    "    'output': {\n",
    "        'model_save_dir': '../saved_models',\n",
    "        'results_save_dir': '../results',\n",
    "        'tensorboard_log_dir': '../logs/tensorboard'\n",
    "    },\n",
    "    'seed': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_processor = DataProcessor(config)\n",
    "data_dict = data_processor.load_data(\n",
    "    config['data']['train_path'],\n",
    "    config['data']['test_path']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(data_dict['X_train'])}\")\n",
    "print(f\"Test samples: {len(data_dict['X_test']) if data_dict['X_test'] is not None else 0}\")\n",
    "print(f\"Number of features: {data_dict['X_train'].shape[1]}\")\n",
    "print(f\"Number of classes: {len(data_dict['class_names'])}\")\n",
    "print(f\"Classes: {data_dict['class_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and loaders\n",
    "train_dataset, val_dataset, test_dataset = data_processor.create_datasets(\n",
    "    data_dict,\n",
    "    validation_split=config['data']['validation_split']\n",
    ")\n",
    "\n",
    "train_loader, val_loader, test_loader = data_processor.create_dataloaders(\n",
    "    train_dataset, val_dataset, test_dataset,\n",
    "    batch_size=config['training']['batch_size']\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "input_dim = data_dict['X_train'].shape[1]\n",
    "num_classes = len(data_dict['class_names'])\n",
    "\n",
    "model = ProteinFTTransformer(\n",
    "    input_dim=input_dim,\n",
    "    num_classes=num_classes,\n",
    "    d_token=config['model']['d_token'],\n",
    "    n_head=config['model']['n_head'],\n",
    "    n_layers=config['model']['n_layers'],\n",
    "    dropout=config['model']['dropout'],\n",
    "    ff_dim_factor=config['model']['ff_dim_factor']\n",
    ").to(device)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer = Trainer(model, train_loader, val_loader, config, device)\n",
    "history = trainer.train()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "from evaluation.metrics import evaluate_model\n",
    "\n",
    "predictions, probabilities, targets = evaluate_model(\n",
    "    model, test_loader, device\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_calculator = MetricsCalculator(num_classes)\n",
    "metrics = metrics_calculator.calculate_all_metrics(\n",
    "    targets, predictions, probabilities\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"  F1-Score (Macro): {metrics['f1_macro']:.4f}\")\n",
    "print(f\"  F1-Score (Weighted): {metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "if num_classes <= 20:\n",
    "    plot_confusion_matrix(targets, predictions, data_dict['class_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "if hasattr(model, 'get_feature_importance'):\n",
    "    # Get a batch of data\n",
    "    sample_batch = next(iter(test_loader))[0][:10].to(device)\n",
    "    importances = model.get_feature_importance(sample_batch)\n",
    "    \n",
    "    # Average importance across samples\n",
    "    avg_importance = importances.mean(dim=0).cpu().numpy()\n",
    "    \n",
    "    # Get top features\n",
    "    top_n = 20\n",
    "    top_indices = np.argsort(avg_importance)[-top_n:][::-1]\n",
    "    \n",
    "    print(f\"\\nTop {top_n} most important features:\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        feature_name = data_dict['feature_names'][idx] if data_dict['feature_names'] else f\"Feature_{idx}\"\n",
    "        print(f\"  {i+1:2d}. {feature_name}: {avg_importance[idx]:.4f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(top_n), avg_importance[top_indices])\n",
    "    plt.xlabel('Feature Rank')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.title(f'Top {top_n} Most Important Features')\n",
    "    plt.xticks(range(top_n), [data_dict['feature_names'][idx] if data_dict['feature_names'] else f\"F{idx}\" for idx in top_indices], rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_save_path = '../saved_models/protein_ft_transformer.pth'\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to: {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
